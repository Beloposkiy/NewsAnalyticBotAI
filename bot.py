import logging
import warnings
import re
from collections import defaultdict

from aiogram import Bot, Dispatcher, F, types
from aiogram.enums import ParseMode
from aiogram.filters import Command
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.types import BotCommand, FSInputFile, InlineKeyboardMarkup, InlineKeyboardButton
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext

from apscheduler.schedulers.asyncio import AsyncIOScheduler

from init_settings.config import BOT_TOKEN, ADMIN_CHAT_ID
from bot_commands.topics import extract_topics
from bot_commands.sentiment import analyze_sentiment
from bot_commands.classifier import classify_topic
from tg.reader import NewsReader
from tg.source import SourceList
from tg.validator import Validator
from bot_commands.pdf_report import generate_pdf
from bot_commands.classifier import CANDIDATE_LABELS

warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub.file_download")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

bot = Bot(token=BOT_TOKEN, parse_mode=ParseMode.HTML)
dp = Dispatcher(storage=MemoryStorage())
scheduler = AsyncIOScheduler()

class SentimentStates(StatesGroup):
    waiting_for_text = State()

async def scheduled_report():
    chat_id = ADMIN_CHAT_ID
    reader = NewsReader()
    await reader.init()
    sources = SourceList()
    validator = Validator(reader.client)
    working_channels, _ = await validator.validate_telegram_channels(sources.get_telegram_channels())
    all_news = []
    for channel in working_channels:
        messages = await reader.telegram_reader(channel, limit=30, days=1)
        all_news.extend(messages)
    top_topics = extract_topics(all_news)
    if not top_topics:
        await bot.send_message(chat_id, "‚ö†Ô∏è –¢–µ–º—ã –Ω–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    pdf_path = generate_pdf(topics=top_topics)
    await bot.send_document(chat_id, FSInputFile(pdf_path), caption="üóÇ –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π PDF-–æ—Ç—á—ë—Ç –ø–æ —Ç–µ–º–∞–º")

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer("üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π. –ò—Å–ø–æ–ª—å–∑—É–π /topics, /sentiment –∏–ª–∏ /report –¥–ª—è –Ω–∞—á–∞–ª–∞.")

@dp.message(SentimentStates.waiting_for_text)
async def process_sentiment(message: types.Message, state: FSMContext):
    result = "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ."
    if message.forward_from_chat and message.text:
        result = analyze_sentiment(message.text.strip())
    elif message.text and message.text.startswith("http") and "t.me" in message.text:
        from bot_commands.sentiment import analyze_telegram_post
        result = await analyze_telegram_post(message.text.strip())
    elif message.text:
        result = analyze_sentiment(message.text.strip())
    await message.answer(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\n{result}")
    await state.clear()

@dp.message(Command("sentiment"))
async def sentiment_cmd(message: types.Message, state: FSMContext):
    await message.answer("‚úçÔ∏è –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç, —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é –∏–ª–∏ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ .txt-—Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏:")
    await state.set_state(SentimentStates.waiting_for_text)

@dp.callback_query(F.data.startswith("select_topic:"))
async def callback_show_full_topic(call: types.CallbackQuery):
    topic_index = int(call.data.split(":")[1])
    user_data = call.message.text.split("\n\n")
    if topic_index < len(user_data):
        await call.message.answer(f"üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–µ–º—ã:\n\n{user_data[topic_index].strip()}")
    await call.answer()

@dp.callback_query(F.data.in_({"generate_pdf_from_topics", "generate_pdf_filtered"}))
async def callback_generate_pdf(call: types.CallbackQuery, state: FSMContext):
    logger.info(f"üìÅ –ö–Ω–æ–ø–∫–∞ PDF –Ω–∞–∂–∞—Ç–∞: {call.data}")
    await call.answer("‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF...")

    data = await state.get_data()
    topics = data.get("filtered_topics") or data.get("all_topics", [])
    category = data.get("current_category", "all")

    if not topics:
        await call.message.answer("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ–º –¥–ª—è –æ—Ç—á—ë—Ç–∞.")
        return

    from datetime import datetime
    date_str = datetime.now().strftime("%d.%m.%Y")
    filename = f"news_{category}_{date_str}_report.pdf"

    pdf_path = generate_pdf(topics=topics, filename=filename)
    await call.message.answer_document(FSInputFile(pdf_path), caption=f"üìÑ PDF-–æ—Ç—á—ë—Ç –ø–æ —Ç–µ–º–∞–º ¬´{category.capitalize()}¬ª")

@dp.message(Command("topics"))
async def topics_cmd(message: types.Message, state: FSMContext):
    logger.info("‚úÖ –ö–æ–º–∞–Ω–¥–∞ /topics –ø–æ–ª—É—á–µ–Ω–∞. –ë–æ—Ç –Ω–∞—á–∞–ª –æ–±—Ä–∞–±–æ—Ç–∫—É.")
    msg = await message.answer("üîç –ü—Ä–æ–≤–µ—Ä—è—é Telegram-–∫–∞–Ω–∞–ª—ã –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–º—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏...")

    try:
        reader = NewsReader()
        await reader.init()
        sources_obj = SourceList()
        telegram_channels = sources_obj.get_telegram_channels()
        validator = Validator(reader.client)
        working_tg, _ = await validator.validate_telegram_channels(telegram_channels)
        all_news = []
        for channel in working_tg:
            messages = await reader.telegram_reader(channel, limit=30, days=1)
            all_news.extend(messages)
        if not all_news:
            await msg.edit_text("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏.")
            return

        top_topics = sorted(extract_topics(all_news), key=lambda t: int(re.search(r"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π: (\d+)", t).group(1)) if re.search(r"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π: (\d+)", t) else 0, reverse=True)[:6]
        if not top_topics:
            await msg.edit_text("üòï –¢–µ–º—ã –Ω–µ –±—ã–ª–∏ –≤—ã–¥–µ–ª–µ–Ω—ã.")
            return

        grouped = defaultdict(list)
        raw_entries = []
        topic_categories = []
        emoji_map = {
            "–ø–æ–ª–∏—Ç–∏–∫–∞": "üèõÔ∏è",
            "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üí∞",
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "üíª",
            "–∫—É–ª—å—Ç—É—Ä–∞": "üé≠",
            "–ø—Ä–æ—á–µ–µ": "üóÇÔ∏è"
        }

        for raw in top_topics:
            cleaned = re.sub(r'[^\w\s.,:;!?‚Äì‚Äî()\"\'¬´¬ª‚Ññ@/%\\-]', '', raw)
            lines = cleaned.strip().split("\n")
            if not lines:
                continue
            header_text = lines[0].lstrip("‚Ä¢").strip()
            category = classify_topic(header_text).lower()
            topic_categories.append(category)
            header = f"üì∞ {header_text}"
            link = next((l.strip() for l in lines if "http" in l or "t.me/" in l), None)
            count_line = next((l.strip() for l in lines if "–£–ø–æ–º–∏–Ω–∞–Ω–∏–π" in l), None)
            count_str = "0"
            if count_line:
                match = re.search(r"\d+", count_line)
                if match:
                    count_str = match.group(0)
            sentiment = analyze_sentiment("\n".join(lines))
            entry = f"{header}"
            if link:
                entry += f"\nüîó {link}"
            entry += f"\nüó£Ô∏è –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {count_str}"
            entry += f"\nüß† –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}"
            entry += "\n\u2063"
            grouped[category].append(entry.strip())
            raw_entries.append(entry.strip())

        full_text = "\n\n".join(raw_entries).strip()

        # —Å–æ–∑–¥–∞—ë–º –∫–Ω–æ–ø–∫–∏ –ø–æ —Å–ø–∏—Å–∫—É –ª–µ–π–±–ª–æ–≤
        topic_buttons = []
        for label in CANDIDATE_LABELS:
            emoji = emoji_map.get(label.lower(), "üóÇÔ∏è")
            topic_buttons.append(
                InlineKeyboardButton(text=f"{emoji} {label.capitalize()}",
                                     callback_data=f"filter_category:{label.lower()}")
            )

        inline_rows = [topic_buttons[i:i + 2] for i in range(0, len(topic_buttons), 2)]
        inline_rows.append([
            InlineKeyboardButton(text="üñ®Ô∏è –°–∫–∞—á–∞—Ç—å PDF-–æ—Ç—á—ë—Ç", callback_data="generate_pdf_from_topics")
        ])
        markup = InlineKeyboardMarkup(inline_keyboard=inline_rows)

        await state.set_data({
            "all_topics": raw_entries,
            "topic_categories": [classify_topic(re.search(r'üì∞ (.+)', t).group(1)) for t in raw_entries]
        })

        await msg.delete()
        await message.answer(full_text, parse_mode="HTML", disable_web_page_preview=True)
        await message.answer("üëá –í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:", reply_markup=markup)

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ Telegram-–∫–∞–Ω–∞–ª–æ–≤:")
        await msg.edit_text(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

@dp.callback_query(F.data.startswith("filter_category:"))
async def filter_by_category(call: types.CallbackQuery, state: FSMContext):
    cat = call.data.split(":")[1]
    logger.info(f"üìÇ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–±—Ä–∞–ª –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {cat}")
    data = await state.get_data()
    all_topics = data.get("all_topics", [])
    categories = data.get("topic_categories", [])

    filtered = [t for t, c in zip(all_topics, categories) if c.lower() == cat.lower()]

    if not filtered:
        await call.message.answer(f"‚ö†Ô∏è –ù–µ—Ç —Ç–µ–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´{cat.capitalize()}¬ª.")
        return

    preview = "\n\n".join(filtered[:5])
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–ø–∏–∫–∏
    await state.update_data(filtered_topics=filtered[:5], current_category=cat)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º 5 –Ω–æ–≤–æ—Å—Ç–µ–π –∏ 2 –∫–Ω–æ–ø–∫–∏
    await call.message.answer(
        f"üìÇ –¢–æ–ø-5 –Ω–æ–≤–æ—Å—Ç–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´{cat.capitalize()}¬ª:\n\n{preview}",
        parse_mode="HTML",
        reply_markup=InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="üîô –ù–∞–∑–∞–¥", callback_data="back_to_all")],
            [InlineKeyboardButton(text="üñ®Ô∏è PDF —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏", callback_data="generate_pdf_filtered")]
        ])
    )
    await call.answer()


@dp.message(Command("report"))
async def report_cmd(message: types.Message):
    await message.answer("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF-–æ—Ç—á—ë—Ç–∞ –ø–æ —Ç–µ–º–∞–º...")
    try:
        reader = NewsReader()
        await reader.init()
        sources = SourceList()
        validator = Validator(reader.client)
        working_channels, _ = await validator.validate_telegram_channels(sources.get_telegram_channels())
        all_news = []
        for channel in working_channels:
            messages = await reader.telegram_reader(channel, limit=30, days=1)
            all_news.extend(messages)
        if not all_news:
            await message.answer("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏.")
            return
        top_topics = extract_topics(all_news)
        if not top_topics:
            await message.answer("üòï –¢–µ–º—ã –Ω–µ –±—ã–ª–∏ –≤—ã–¥–µ–ª–µ–Ω—ã.")
            return
        pdf_path = generate_pdf(topics=top_topics)
        await message.answer_document(FSInputFile(pdf_path), caption="üóÇ –í–∞—à PDF-–æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤!")
    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞:")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞: {e}")

@dp.callback_query(F.data == "back_to_all")
async def back_to_all(call: types.CallbackQuery, state: FSMContext):
    logger.info("üîô –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–µ—Ä–Ω—É–ª—Å—è –∫ —Å–ø–∏—Å–∫—É –≤—Å–µ—Ö —Ç–µ–º")
    data = await state.get_data()
    topics = data.get("all_topics", [])

    if not topics:
        await call.message.answer("‚ö†Ô∏è –¢–µ–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    full_text = "\n\n".join(topics).strip()

    from bot_commands.classifier import CANDIDATE_LABELS
    emoji_map = {
        "–ø–æ–ª–∏—Ç–∏–∫–∞": "üèõÔ∏è", "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üí∞", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "üíª",
        "–∫—É–ª—å—Ç—É—Ä–∞": "üé≠", "–ø—Ä–æ—á–µ–µ": "üóÇÔ∏è"
    }

    topic_buttons = []
    for label in CANDIDATE_LABELS:
        emoji = emoji_map.get(label.lower(), "üóÇÔ∏è")
        topic_buttons.append(
            InlineKeyboardButton(text=f"{emoji} {label.capitalize()}", callback_data=f"filter_category:{label.lower()}")
        )

    rows = [topic_buttons[i:i+2] for i in range(0, len(topic_buttons), 2)]
    rows.append([InlineKeyboardButton(text="üñ®Ô∏è –°–∫–∞—á–∞—Ç—å PDF-–æ—Ç—á—ë—Ç", callback_data="generate_pdf_from_topics")])
    markup = InlineKeyboardMarkup(inline_keyboard=rows)

    await call.message.answer(full_text, parse_mode="HTML", disable_web_page_preview=True)
    await call.message.answer("üëá –í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:", reply_markup=markup)
    await call.answer()


@dp.message(F.text & ~F.text.startswith("/"))
async def debug_log(message: types.Message):
    logger.info(f"üí¨ –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {message.text}")

async def main():
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    try:
        await bot.delete_webhook(drop_pending_updates=True)
        await bot.set_my_commands([
            BotCommand(command="start", description="–ó–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞"),
            BotCommand(command="topics", description="–ü–æ–∫–∞–∑–∞—Ç—å –≥–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã"),
            BotCommand(command="sentiment", description="–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞/—Å—Å—ã–ª–∫–∏/—Ñ–∞–π–ª–∞"),
            BotCommand(command="report", description="–°–æ–∑–¥–∞—Ç—å PDF-–æ—Ç—á—ë—Ç"),
        ])
        scheduler.add_job(scheduled_report, trigger="cron", hour=10, minute=0)
        scheduler.start()
        logger.info("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        await dp.start_polling(bot)
    except KeyboardInterrupt:
        print("üõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    finally:
        scheduler.shutdown(wait=False)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

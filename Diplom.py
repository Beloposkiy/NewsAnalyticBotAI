import os
import types
from datetime import datetime, timezone, timedelta

import dp
from aiogram.filters import Command
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import StatesGroup, State
from telethon import TelegramClient
from telethon.errors import UsernameInvalidError, UsernameNotOccupiedError, ChannelPrivateError
from telethon.tl.functions.messages import GetHistoryRequest

from bot import SentimentStates, generate_and_send_report, logger
from bot_commands.sentiment import analyze_sentiment, run_model
from bot_commands.topics import extract_topics
from bot_commands.utils import log_user_action, get_category_buttons, format_topic_block
from init_settings.config import api_id, api_hash
from tg.reader import NewsReader
from tg.source import SourceList
from tg.validator import Validator


import re
import logging
import torch
import numpy as np
from aiogram import Bot, Dispatcher, types
from aiogram.filters import Command
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.storage.memory import MemoryStorage
from telethon import TelegramClient
from telethon.errors import MessageIdInvalidError
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from init_settings.config import BOT_TOKEN, api_id, api_hash, session_path

# --- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s]: %(message)s",
    handlers=[
        logging.FileHandler("newsbot.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ ---
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher(storage=MemoryStorage())

# --- –°–æ—Å—Ç–æ—è–Ω–∏—è FSM ---
class SentimentStates(StatesGroup):
    waiting_for_text = State()

# --- –ú–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ ---
base_model_name = "blanchefort/rubert-base-cased-sentiment"
base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)
labels = ['negative', 'neutral', 'positive']

fallback_model_name = "cointegrated/rubert-tiny-sentiment-balanced"
fallback_tokenizer = AutoTokenizer.from_pretrained(fallback_model_name)
fallback_model = AutoModelForSequenceClassification.from_pretrained(fallback_model_name)

NEGATIVE_KEYWORDS = [
    "–ø—Ä–æ–≤–∞–ª", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ", "—Å–∞–Ω–∫—Ü–∏", "–Ω–µ –±—É–¥–µ—Ç", "–æ—Ç–∫–∞–∑",
    "–∞–≤–∞—Ä–∏—è", "—Å–Ω–∏–∑–∏–ª—Å—è", "–±–∞–∫—Ç–µ—Ä–∏–∏", "–∑–∞—Ä–∞–∂—ë–Ω", "–∫–∏—à–µ—á–Ω–∞—è –ø–∞–ª–æ—á–∫–∞",
    "–∏–Ω—Ñ–µ–∫—Ü–∏—è", "–æ—Ç—Ä–∞–≤–ª–µ–Ω–∏–µ"
]

# --- –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Å –º–æ–¥–µ–ª—å—é ---
def run_model(text: str, tokenizer, model) -> tuple[str, float]:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        logits = model(**inputs).logits
    probs = torch.nn.functional.softmax(logits, dim=-1).squeeze().numpy()
    top = int(np.argmax(probs))
    return labels[top], probs[top]

# --- –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º–∏ ---
def analyze_sentiment(text: str) -> str:
    try:
        text_lower = text.lower()
        # –†—É—á–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        if any(word in text_lower for word in NEGATIVE_KEYWORDS):
            return "üò† –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)"

        label, score = run_model(text, base_tokenizer, base_model)
        if score < 0.75:
            label, score = run_model(text, fallback_tokenizer, fallback_model)

        if label == "neutral" and any(word in text_lower for word in NEGATIVE_KEYWORDS):
            label = "negative"

        if label == "positive":
            return f"üòä –ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è ({score:.2f})"
        elif label == "neutral":
            return f"üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è ({score:.2f})"
        elif label == "negative":
            return f"üò† –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è ({score:.2f})"
        else:
            return "ü§∑ –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Telegram-–ø–æ—Å—Ç–∞ ---
async def extract_text_from_telegram(url: str) -> str:
    match = re.match(r"https?://t\.me/(\w+)/(\d+)", url)
    if not match:
        return "‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Telegram-–ø–æ—Å—Ç."

    channel_username, msg_id = match.groups()
    msg_id = int(msg_id)

    try:
        async with TelegramClient(session_path, api_id, api_hash) as client:
            message = await client.get_messages(channel_username, ids=msg_id)
            if not message:
                return "[‚õî –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ]"
            parts = [
                message.text or "",
                getattr(message, "message", ""),
                getattr(message, "caption", ""),
            ]
            combined = "\n".join(set(filter(None, map(str.strip, parts))))
            logger.info(f"[–¢–µ–∫—Å—Ç Telegram-–ø–æ—Å—Ç–∞]: {combined}")
            return combined if combined else "[–ü–æ—Å—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω]"
    except MessageIdInvalidError:
        return "[‚õî –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ]"
    except Exception as e:
        logger.warning(f"[–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ Telegram]: {e}")
        return f"[‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ Telegram: {e}]"

# --- –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–∞ –ø–æ —Å—Å—ã–ª–∫–µ ---
async def analyze_telegram_post(url: str) -> str:
    text = await extract_text_from_telegram(url)
    if text.startswith("[") or text.startswith("‚ö†Ô∏è"):
        return text
    return analyze_sentiment(text)

# --- –•–µ–Ω–¥–ª–µ—Ä –∫–æ–º–∞–Ω–¥—ã /sentiment ---
@dp.message(Command("sentiment"))
async def sentiment_cmd(message: types.Message, state: FSMContext):
    log_user_action(message.from_user.id, message.from_user.username, message.from_user.full_name, "–ö–æ–º–∞–Ω–¥–∞ /sentiment")
    await message.answer("‚úçÔ∏è –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç, —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é –∏–ª–∏ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ .txt-—Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏:")
    await state.set_state(SentimentStates.waiting_for_text)

# --- –•–µ–Ω–¥–ª–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —Å—Å—ã–ª–∫–∏ ---
@dp.message(SentimentStates.waiting_for_text)
async def process_sentiment(message: types.Message, state: FSMContext):
    text = message.text.strip()

    log_user_action(
        user_id=message.from_user.id,
        username=message.from_user.username,
        full_name=message.from_user.full_name,
        action="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"
    )

    if message.forward_from_chat and text:
        result = analyze_sentiment(text)
    elif text.startswith("http") and "t.me" in text:
        result = await analyze_telegram_post(text)
    else:
        result = analyze_sentiment(text)

    await message.answer(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:\n{result}")
    await state.clear()



import re
import unicodedata
import torch
import numpy as np
import logging
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from telethon import TelegramClient
from telethon.errors import MessageIdInvalidError
from init_settings.config import api_id, api_hash, session_path

logger = logging.getLogger(__name__)

# === –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å: Blanchefort (—Ç–æ—á–Ω–µ–µ, –Ω–æ —Ç—è–∂–µ–ª–µ–µ) ===
base_model_name = "blanchefort/rubert-base-cased-sentiment"
base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)
labels = ['negative', 'neutral', 'positive']

# === –†–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ–¥–µ–ª—å: Cointegrated (–ª—ë–≥–∫–∞—è) ===
fallback_model_name = "cointegrated/rubert-tiny-sentiment-balanced"
fallback_tokenizer = AutoTokenizer.from_pretrained(fallback_model_name)
fallback_model = AutoModelForSequenceClassification.from_pretrained(fallback_model_name)

# === –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—á–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ ===
NEGATIVE_KEYWORDS = ["–ø—Ä–æ–≤–∞–ª", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ", "—Å–∞–Ω–∫—Ü–∏", "–Ω–µ –±—É–¥–µ—Ç", "–æ—Ç–∫–∞–∑", "–∞–≤–∞—Ä–∏—è", "—Å–Ω–∏–∑–∏–ª—Å—è"]

def clean_and_trim(text: str, max_length: int = 200) -> str:
    # –£–¥–∞–ª–µ–Ω–∏–µ emoji
    emoji_pattern = re.compile(
        "[" +
        u"\U0001F600-\U0001F64F" +
        u"\U0001F300-\U0001F5FF" +
        u"\U0001F680-\U0001F6FF" +
        u"\U0001F1E0-\U0001F1FF" +
        u"\u2600-\u26FF" +
        u"\u2700-\u27BF" +
        "]+", flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)

    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–∞ –∑–∞–º–µ–Ω—ã –∏ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = text.replace('\uFFFD', '')  # —Å–∏–º–≤–æ–ª ÔøΩ
    text = ''.join(c for c in text if unicodedata.category(c)[0] != 'C')

    return text.strip()[:max_length]

def run_model(text: str, tokenizer, model) -> tuple[str, float]:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        logits = model(**inputs).logits
    probs = torch.nn.functional.softmax(logits, dim=-1).squeeze().numpy()
    top = int(np.argmax(probs))
    return labels[top], probs[top]

def analyze_sentiment(text: str) -> str:
    try:
        text_lower = text.lower()
        # –†—É—á–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        if any(word in text_lower for word in NEGATIVE_KEYWORDS):
            return "üò† –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)"

        label, score = run_model(text, base_tokenizer, base_model)
        if score < 0.75:
            label, score = run_model(text, fallback_tokenizer, fallback_model)

        if label == "neutral" and any(word in text_lower for word in NEGATIVE_KEYWORDS):
            label = "negative"

        if label == "positive":
            return f"üòä –ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è ({score:.2f})"
        elif label == "neutral":
            return f"üòê –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è ({score:.2f})"
        elif label == "negative":
            return f"üò† –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è ({score:.2f})"
        else:
            return "ü§∑ –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"

async def extract_text_from_telegram(url: str) -> str:
    match = re.match(r"https?://t\.me/(\w+)/(\d+)", url)
    if not match:
        return "‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Telegram-–ø–æ—Å—Ç."

    channel_username, msg_id = match.groups()
    msg_id = int(msg_id)

    try:
        async with TelegramClient(session_path, api_id, api_hash) as client:
            message = await client.get_messages(channel_username, ids=msg_id)
            if not message:
                return "[‚õî –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ]"

            parts = []
            for attr in ('message', 'caption', 'text', 'raw_text'):
                val = getattr(message, attr, None)
                if val and isinstance(val, str) and val.strip():
                    parts.append(val.strip())

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            seen = set()
            unique_parts = []
            for part in parts:
                if part not in seen:
                    unique_parts.append(part)
                    seen.add(part)

            combined_text = "\n\n".join(unique_parts)
            logger.info(f"[–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç Telegram-–ø–æ—Å—Ç–∞]: {combined_text}")
            return combined_text if combined_text else "[–ü–æ—Å—Ç –ø—É—Å—Ç –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –º–µ–¥–∏–∞]"

    except MessageIdInvalidError:
        return "[‚õî –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ]"
    except Exception as e:
        logger.warning(f"[–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ Telegram]: {e}")
        return f"[‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ Telegram: {e}]"

async def analyze_telegram_post(url: str) -> str:
    full_text = await extract_text_from_telegram(url)
    cleaned_text = clean_and_trim(full_text)
    logger.info(f"[–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞]: {cleaned_text}")
    if cleaned_text.startswith("[") or cleaned_text.startswith("‚ö†Ô∏è"):
        return cleaned_text
    return analyze_sentiment(cleaned_text)
